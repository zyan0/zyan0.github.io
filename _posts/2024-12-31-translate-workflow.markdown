---
layout: post
title:  "End-to-End Workflow: Transcribe, Generate SRT, and Translate Japanese Audio to Chinese Subtitles"
date:   2024-12-31 17:56:54 -0800
categories: posts
---

This blog post covers a full pipeline to process **audio content in any language** and produce **subtitles in a target language** with minimal code adjustments and uses Japanese to Chinese as an example.
All the codes are written in Python and generated by ChatGPT o1.
(Actually this blog post is generated by ChatGPT o1 and 4o as well.)

1. **Extract/Transcribe Audio**: Using [OpenAI’s Whisper API](https://platform.openai.com/docs/guides/speech-to-text) to convert audio into text (with timestamps).
2. **Generate an SRT**: Convert those timestamped segments into an SRT file.
3. **Translate** the SRT from Japanese to Chinese: Use GPT-4 for chunk-based and context-aware translations.
4. **Embed subtitles into video**: Use `ffmpeg` to embed subtitles into a video.

We’ll also show you how to verify translations for completeness, ensuring a high-quality final subtitle track.

## Prerequisites

- **Python 3.9+**
- **Pip-installed** packages:
  - `openai>=1.0.0`
  - `pydub`
  - `tqdm`
  - `python-srt`
- An **OpenAI API key**, which can be obtained by signing up at [OpenAI’s platform](https://platform.openai.com/signup/), and then setting it in your environment:
  ```bash
  export OPENAI_API_KEY="..."
  ```

---

# 1. Transcribing Audio With `transcribe_audio.py`

Often, your **audio file** might be too large (>25 MB) for the OpenAI Whisper API. So we **split** it into smaller chunks (e.g. 10 minutes each) and call `openai.Audio.transcribe` on each chunk.

Below is an example script: **`transcribe_audio.py`**.

```python
#!/usr/bin/env python3

import argparse
import os
import json
from pydub import AudioSegment
import openai
from tqdm import tqdm

def split_audio(input_file: str, chunk_length_minutes: float) -> list[str]:
    """
    Splits the input audio file into multiple chunks of `chunk_length_minutes`.
    Saves each chunk as a separate .mp3 file in the same directory as the input.
    Returns a list of chunk file paths.
    """
    audio = AudioSegment.from_file(input_file)
    chunk_length_ms = int(chunk_length_minutes * 60 * 1000)

    base_name = os.path.splitext(os.path.basename(input_file))[0]
    directory = os.path.dirname(os.path.abspath(input_file))
    
    chunks = []
    start = 0
    end = len(audio)
    chunk_count = 0

    while start < end:
        chunk_count += 1
        chunk_audio = audio[start:start + chunk_length_ms]
        chunk_path = os.path.join(directory, f"{base_name}_chunk_{chunk_count}.mp3")
        chunk_audio.export(chunk_path, format="mp3")
        chunks.append(chunk_path)
        start += chunk_length_ms

    return chunks

def transcribe_file(file_path: str, model: str = "whisper-1", language: str = "ja"):
    """
    Transcribes a single audio file using OpenAI Whisper API (up to 25 MB).
    Returns the verbose_json response as a Python dict.
    """
    if not openai.api_key:
        raise ValueError("No OPENAI_API_KEY found in environment variables.")

    with open(file_path, "rb") as audio_file:
        response = openai.Audio.transcribe(
            file=audio_file,
            model=model,
            language=language,
            response_format="verbose_json",
            timestamp_granularity="segment"
        )
    return response

def main():
    parser = argparse.ArgumentParser(
        description="Split a large audio file into smaller chunks, then transcribe each chunk with OpenAI Whisper."
    )
    parser.add_argument("--input", required=True, help="Path to the input audio file (e.g., .mp3, .m4a, .wav).")
    parser.add_argument("--chunk_minutes", type=float, default=10.0,
                        help="Length (in minutes) of each chunk. Default: 10 minutes.")
    parser.add_argument("--model", default="whisper-1", help="OpenAI Whisper model to use (default: whisper-1).")
    parser.add_argument("--language", default="ja", help="Language in ISO-639-1 code (e.g., 'ja' for Japanese).")
    parser.add_argument("--output_json", default="transcript.json",
                        help="Path to save the combined transcription JSON.")
    args = parser.parse_args()

    openai.api_key = os.environ.get("OPENAI_API_KEY")
    if not openai.api_key:
        raise ValueError("No OPENAI_API_KEY found in environment variables.")

    # 1) Split the audio
    print(f"Splitting '{args.input}' into chunks of {args.chunk_minutes} minute(s)...")
    chunk_paths = split_audio(args.input, args.chunk_minutes)
    print(f"Created {len(chunk_paths)} chunk file(s).")

    # 2) Transcribe each chunk
    combined_results = []
    for chunk_path in tqdm(chunk_paths, desc="Transcribing chunks"):
        try:
            result = transcribe_file(chunk_path, model=args.model, language=args.language)
            combined_results.append({
                "chunk_file": chunk_path,
                "transcription": result
            })
        except openai.error.OpenAIError as e:
            print(f"[ERROR] Failed to transcribe {chunk_path}: {e}")

    # 3) Save the combined results to JSON
    with open(args.output_json, "w", encoding="utf-8") as f:
        json.dump(combined_results, f, ensure_ascii=False, indent=2)
    print(f"\nDone! Combined transcription saved to {args.output_json}")

if __name__ == "__main__":
    main()
```

---

# 2. Generating SRT With `generate_srt.py`

Once you have the **transcript JSON** from the previous step, you typically have one or more chunks, each containing a `transcription` object. To form a **single SRT**, we gather all segments, apply any offset, and then write them in chronological order.

```python
#!/usr/bin/env python3

import srt
from datetime import timedelta
import json
import argparse
import os

def segments_to_srt(segments, out_path="output_subtitles.srt"):
    """
    segments = [
      {"start": 1.23, "end": 5.67, "text": "some line"},
      {"start": 5.67, "end": 10.00, "text": "next line"},
      ...
    ]
    Writes them to an SRT file at out_path.
    """
    subtitles = []
    for i, seg in enumerate(segments, start=1):
        start_td = timedelta(seconds=seg["start"])
        end_td = timedelta(seconds=seg["end"])
        subtitles.append(
            srt.Subtitle(index=i, start=start_td, end=end_td, content=seg["text"])
        )

    srt_text = srt.compose(subtitles)
    with open(out_path, "w", encoding="utf-8") as f:
        f.write(srt_text)
    print(f"SRT saved to {out_path}")

def load_transcript_json(json_path):
    """
    Loads the JSON file from transcribe_audio.py.
    Returns a merged list of segments.
    """
    with open(json_path, "r", encoding="utf-8") as f:
        chunked_data = json.load(f)

    final_segments = []
    for chunk_info in chunked_data:
        trans_segments = chunk_info["transcription"]["segments"]
        final_segments.extend(trans_segments)

    final_segments.sort(key=lambda s: s["start"])
    return final_segments

def main():
    parser = argparse.ArgumentParser(
        description="Generate a single SRT from the JSON output of chunked Whisper transcriptions."
    )
    parser.add_argument("--input_json", required=True, help="Path to the combined transcript JSON")
    parser.add_argument("--output_srt", default="output_subtitles.srt", help="Path for the generated SRT")
    args = parser.parse_args()

    # Load & merge segments
    segments = load_transcript_json(args.input_json)

    # Convert segments to a single SRT
    segments_to_srt(segments, out_path=args.output_srt)

if __name__ == "__main__":
    main()
```

---

# 3. Translating Japanese SRT to Chinese With `translate_srt_jp_to_cn.py`

Finally, you can run a **chunk-based** translation on the generated SRT. Below is a script that handles the main translation process, leveraging GPT-4 for high-quality output.

```python
#!/usr/bin/env python3

import openai
import argparse
import os

def translate_srt(input_srt, output_srt, model="gpt-4", language_pair=("ja", "zh")):
    """
    Translates the input SRT file from Japanese to Chinese.
    """
    with open(input_srt, "r", encoding="utf-8") as f:
        srt_text = f.read()

    response = openai.ChatCompletion.create(
        model=model,
        messages=[
            {"role": "system", "content": "Translate subtitles from Japanese to Chinese."},
            {"role": "user", "content": srt_text}
        ],
        temperature=0.2,
    )

    translated_srt = response.choices[0].message.content

    with open(output_srt, "w", encoding="utf-8") as f:
        f.write(translated_srt)
    print(f"Translated SRT saved to {output_srt}")

def main():
    parser = argparse.ArgumentParser(
        description="Translate a Japanese SRT file to Chinese using GPT-4."
    )
    parser.add_argument("--input_srt", required=True, help="Path to the input SRT file (Japanese).")
    parser.add_argument("--output_srt", required=True, help="Path to save the output SRT file (Chinese).")
    args = parser.parse_args()

    openai.api_key = os.environ.get("OPENAI_API_KEY")
    if not openai.api_key:
        raise ValueError("No OPENAI_API_KEY found in environment variables.")

    translate_srt(args.input_srt, args.output_srt)

if __name__ == "__main__":
    main()
```

---

## 4. Combine SRT into Video

To embed subtitles into a video, you can use the `ffmpeg` tool, which is powerful and versatile for video processing tasks. Below is an example command to add your subtitles:

```
ffmpeg \
  -i input_video.mp4 \
  -i "big_subtitles_cn.srt" \
  -map 0:v -map 0:a -map 1:0 \
  -c copy -c:s mov_text \
  -metadata:s:s:0 language=zho \
  -disposition:s:0 default \
  output_video.mp4
```

### Explanation of Parameters:
- **`-i input_video.mp4`**: Specifies the input video file.
- **`-i "big_subtitles_cn.srt"`**: Specifies the SRT subtitle file.
- **`-map 0:v -map 0:a -map 1:0`**: Maps the video, audio, and subtitle streams to the output file.
- **`-c copy`**: Copies the video and audio streams without re-encoding them.
- **`-c:s mov_text`**: Specifies the subtitle codec, compatible with MP4 files.
- **`-metadata:s:s:0 language=zho`**: Sets the language metadata of the subtitle stream to Chinese (`zho`).
- **`-disposition:s:0 default`**: Marks the subtitle stream as the default.

### Tips:
- Ensure the SRT file is properly formatted and synced with the video duration.
- If you encounter issues with unsupported subtitle codecs, consider converting the SRT file to another format (e.g., ASS) or using a different container format like MKV.

### Output:
This command produces a new video file (`output_video.mp4`) with embedded subtitles, ready for playback on most media players.

---

## Putting it all together

Let's put it all together and run the following command:

```bash
python transcribe_audio.py \
  --input input_video.mp4 \
  --chunk_minutes 10 \
  --model whisper-1 \
  --language ja \
  --output_json transcript.json

python generate_srt.py \
  --input_json transcript.json \
  --output_srt output_subtitles.srt

python translate_srt_jp_to_cn.py \
  --input_srt output_subtitles.srt \
  --output_srt big_subtitles_cn.srt

ffmpeg \
  -i input_video.mp4 \
  -i "big_subtitles_cn.srt" \
  -map 0:v -map 0:a -map 1:0 \
  -c copy -c:s mov_text \
  -metadata:s:s:0 language=zho \
  -disposition:s:0 default \
  output_video.mp4
```


---

# Conclusion

This end-to-end solution leverages **OpenAI Whisper** for **speech-to-text** and **GPT-4o** for **high-quality translations**. By chunking large audio files, generating accurate transcriptions, and translating contextually, you can produce **coherent, accurate subtitles** in Chinese from Japanese video content.

Feel free to adapt these scripts to your workflow. With these tools, you can **localize** Japanese content efficiently!

